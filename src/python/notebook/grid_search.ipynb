{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from time import time\n",
    "import datetime\n",
    "from pandas.core.common import flatten\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Parsing and pre-processing\n",
    "import glob, re, os, sys, random\n",
    "from random import shuffle\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Vector representations and embeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim\n",
    "\n",
    "# Modeling - Logistic, XGBOOST, SVM\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(r\"../../../data/processed/pre-processed_2023_03_11.json\")\n",
    "\n",
    "df.drop_duplicates(subset=['article_new', 'case_num'], keep='first', inplace=True)\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase1 vs. Phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['phase2'].isin([0,1])].reset_index(drop=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df1['label'] = le.fit_transform(df1['phase2'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance data\n",
    "- undersample minority class\n",
    "- use excluded cases as test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1485\n",
      "1      98\n",
      "Name: case_num, dtype: int64 \n",
      " 0    1485\n",
      "1      98\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df1.groupby('label')['case_num'].nunique(), \"\\n\",\n",
    "df1['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total decisions: 1583\n",
      "0    1485\n",
      "1      98\n",
      "Name: label, dtype: int64\n",
      "article6(1)(b)    1284\n",
      "article6(2)        201\n",
      "article8(2)         57\n",
      "article8(1)         32\n",
      "article8(3)          9\n",
      "Name: article_new, dtype: int64\n",
      "   label case_num     article_new\n",
      "0      0  M.10001  article6(1)(b)\n"
     ]
    }
   ],
   "source": [
    "# Get unique number of decisions\n",
    "df_unique = df1.groupby(['label', 'case_num','article_new']).first().reset_index()[['label', 'case_num', 'article_new']]\n",
    "print(\"Total decisions:\", len(df_unique.index))\n",
    "print(df_unique['label'].value_counts())\n",
    "print(df_unique['article_new'].value_counts())\n",
    "print(df_unique.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def balance(decision_id, Ytrain, random_seed):\n",
    "    print('Balancing...')\n",
    "    v = [i for i, val in enumerate(Ytrain) if val == 1]\n",
    "    nv = [i for i, val in enumerate(Ytrain) if val == 0]\n",
    "\n",
    "    if len(nv) < len(v):\n",
    "        v = random.sample(v, len(nv))\n",
    "    else:\n",
    "        nv = random.sample(nv, len(v))\n",
    "\n",
    "    indices = v + nv\n",
    "    random.seed(random_seed)  # Set the random seed\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    decision_id = [decision_id[i] for i in indices]\n",
    "    Ytrain = [Ytrain[i] for i in indices]\n",
    "\n",
    "    print(\"Total decisions:\", len(decision_id))\n",
    "    print(\"Labels distribution:\", \"\\n\", (pd.DataFrame(Ytrain)[0].value_counts()))\n",
    "    return decision_id, Ytrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing...\n",
      "Total decisions: 196\n",
      "Labels distribution: \n",
      " 0    98\n",
      "1    98\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "decision_id_rus, y_train_rus = balance(df_unique['case_num'], df_unique['label'], random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df1[df1['case_num'].isin(decision_id_rus) & df1['label'].isin(y_train_rus)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1386\n",
      "Name: case_num, dtype: int64 \n",
      " 0    1386\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_excluded = df1[~(df1['case_num'].isin(decision_id_rus) & df1['label'].isin(y_train_rus))]\n",
    "print(df_excluded.groupby('label')['case_num'].nunique(), \"\\n\",\n",
    "df_excluded['label'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split balanced data into train (80%) and test (20%)\n",
    "- Match case_num and phase2 to get Train set in df1\n",
    "- Use df_excluded to test the system(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (157,) (157, 2)\n",
      "Test set shape: (40,) (40, 2)\n"
     ]
    }
   ],
   "source": [
    "# Separate the data into features (text) and labels (phase and section_fin)\n",
    "X = df_balanced['text_clean']\n",
    "y = df_balanced[['label', 'article_new']]\n",
    "case_num = df_balanced[['case_num', 'year', 'section_fin', 'file']]\n",
    "\n",
    "# Split the data into train and test sets based on phase2 and section_fin\n",
    "X_train, X_test, y_train, y_test, case_num_train, case_num_test = train_test_split(X, y, case_num, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Print the shape of each set to verify that the data has been split correctly\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    79\n",
      "1    78\n",
      "Name: case_num, dtype: int64 \n",
      " Total rows: 157 \n",
      " 0    79\n",
      "1    78\n",
      "Name: label, dtype: int64\n",
      "label\n",
      "0    20\n",
      "1    20\n",
      "Name: case_num, dtype: int64 \n",
      " Total rows: 40 \n",
      " 0    20\n",
      "1    20\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Concatenate X_train, y_train, and case_num_train along axis=1\n",
    "df_train = pd.concat([X_train, y_train, case_num_train], axis=1)\n",
    "\n",
    "# Concatenate X_test, y_test, and case_num_test along axis=1\n",
    "df_test1 = pd.concat([X_test, y_test, case_num_test], axis=1)\n",
    "\n",
    "print(df_train.groupby('label')['case_num'].nunique(), \"\\n\",\n",
    "    \"Total rows:\", len(df_train.index), \"\\n\",\n",
    "    df_train['label'].value_counts())\n",
    "\n",
    "print(df_test1.groupby('label')['case_num'].nunique(), \"\\n\",\n",
    "    \"Total rows:\", len(df_test1.index), \"\\n\",\n",
    "    df_test1['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases label\n",
      "0    79\n",
      "1    78\n",
      "Name: case_num, dtype: int64 \n",
      " Total rows: 157 \n",
      " Rows 0    79\n",
      "1    78\n",
      "Name: label, dtype: int64\n",
      "Cases label\n",
      "0    1406\n",
      "1      20\n",
      "Name: case_num, dtype: int64 \n",
      " Total rows: 1426 \n",
      " Rows 0    1406\n",
      "1      20\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Add excluded cases to df_test\n",
    "df_concat = df_excluded[['text_clean', 'label', 'article_new', 'case_num', 'year', 'section_fin', 'file']]\n",
    "df_test = pd.concat([df_test1, df_concat], axis=0)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "print(\"Cases\", df_train.groupby('label')['case_num'].nunique(), \"\\n\",\n",
    "    \"Total rows:\", len(df_train.index), \"\\n\",\n",
    "    \"Rows\", df_train['label'].value_counts())\n",
    "\n",
    "print(\"Cases\", df_test.groupby('label')['case_num'].nunique(), \"\\n\",\n",
    "    \"Total rows:\", len(df_test.index), \"\\n\",\n",
    "    \"Rows\", df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    79\n",
      "1    78\n",
      "Name: case_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group df_train by 'case_num', 'article_new', 'file', and join the 'text_clean' column\n",
    "df_train_grouped = df_train.groupby(['year', 'case_num', 'file', 'article_new', 'label'])['text_clean'].agg(' '.join).reset_index()\n",
    "print(df_train_grouped.groupby('label')['case_num'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>case_num</th>\n",
       "      <th>file</th>\n",
       "      <th>article_new</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004</td>\n",
       "      <td>M.3476</td>\n",
       "      <td>\\m3476_20041028_20310_en</td>\n",
       "      <td>article6(1)(b)</td>\n",
       "      <td>0</td>\n",
       "      <td>horizontal overlap case comp m danish crown st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005</td>\n",
       "      <td>M.3556</td>\n",
       "      <td>\\m3556_20050119_20310_en</td>\n",
       "      <td>article6(1)(b)</td>\n",
       "      <td>0</td>\n",
       "      <td>product market definition joint venture active...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005</td>\n",
       "      <td>M.3595</td>\n",
       "      <td>\\m3595_20050330_20310_en</td>\n",
       "      <td>article6(1)(b)</td>\n",
       "      <td>0</td>\n",
       "      <td>proposed operation constitutes concentration m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year case_num                      file     article_new  label  \\\n",
       "0  2004   M.3476  \\m3476_20041028_20310_en  article6(1)(b)      0   \n",
       "1  2005   M.3556  \\m3556_20050119_20310_en  article6(1)(b)      0   \n",
       "2  2005   M.3595  \\m3595_20050330_20310_en  article6(1)(b)      0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  horizontal overlap case comp m danish crown st...  \n",
       "1  product market definition joint venture active...  \n",
       "2  proposed operation constitutes concentration m...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_grouped.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1406\n",
      "1      20\n",
      "Name: case_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group df_test by 'case_num', 'article_new', 'file', and join the 'text_clean' column\n",
    "df_test_grouped = df_test.groupby(['year', 'article_new', 'case_num', 'file', 'label'])['text_clean'].agg(' '.join).reset_index()\n",
    "print(df_test_grouped.groupby('label')['case_num'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>article_new</th>\n",
       "      <th>case_num</th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004</td>\n",
       "      <td>article6(1)(b)</td>\n",
       "      <td>M.3355</td>\n",
       "      <td>\\m3355_20040615_310_en</td>\n",
       "      <td>0</td>\n",
       "      <td>transmission market assumption narrow market d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004</td>\n",
       "      <td>article6(1)(b)</td>\n",
       "      <td>M.3439</td>\n",
       "      <td>\\m3439_20040809_20310_en</td>\n",
       "      <td>0</td>\n",
       "      <td>proposed concentration involves acquisition so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004</td>\n",
       "      <td>article6(1)(b)</td>\n",
       "      <td>M.3448</td>\n",
       "      <td>\\m3448_en</td>\n",
       "      <td>0</td>\n",
       "      <td>present transaction result edp having sole con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year     article_new case_num                      file  label  \\\n",
       "0  2004  article6(1)(b)   M.3355    \\m3355_20040615_310_en      0   \n",
       "1  2004  article6(1)(b)   M.3439  \\m3439_20040809_20310_en      0   \n",
       "2  2004  article6(1)(b)   M.3448                 \\m3448_en      0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  transmission market assumption narrow market d...  \n",
       "1  proposed concentration involves acquisition so...  \n",
       "2  present transaction result edp having sole con...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_grouped.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline - Logit, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train['text_clean'])\n",
    "y_train = df_train['label']\n",
    "\n",
    "# Transform the test data using the same vectorizer as the training data\n",
    "X_test = vectorizer.transform(df_test['text_clean'])\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 83.31 \n",
      " Precision: 0.053 \n",
      " Recall: 0.650 \n",
      " F1: 0.098\n"
     ]
    }
   ],
   "source": [
    "# Fit the logistic regression model\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) *100.0\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f' Accuracy: {accuracy:.2f} \\n Precision: {precision:.3f} \\n Recall: {recall:.3f} \\n F1: {f_score:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 81.84 \n",
      " Precision: 0.049 \n",
      " Recall: 0.650 \n",
      " F1: 0.091\n"
     ]
    }
   ],
   "source": [
    "# Fit the SVC\n",
    "clf = LinearSVC().fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) *100.0\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f' Accuracy: {accuracy:.2f} \\n Precision: {precision:.3f} \\n Recall: {recall:.3f} \\n F1: {f_score:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search - SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "X_train = df_train['text_clean']\n",
    "y_train = df_train['label']\n",
    "\n",
    "# Combined test set\n",
    "X_test = df_test['text_clean']\n",
    "y_test = df_test['label']\n",
    "\n",
    "# 20% of full data\n",
    "X_test1 = df_test1['text_clean']\n",
    "y_test1 = df_test1['label']\n",
    "\n",
    "# excluded only\n",
    "X_test2 = df_concat['text_clean']\n",
    "y_test2 = df_concat['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 864 candidates, totalling 2592 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LinearSVC\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='word')),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "# Define the grid of hyperparameters to search over\n",
    "parameters = {\n",
    "\t'tfidf__ngram_range': [(1,2),(1,1),(1,3), (2,2),(2,3), (1,4),(2,2),(2,3),(2,4),(3,3),(3,4),(4,4)],\n",
    "\t#'tfidf__analyzer': ('word', 'char'),\n",
    "\t#'tfidf__lowercase': (True, False),\n",
    "\t#'tfidf__max_df': (0.01, 1.0), # ignore words that occur as more than 1% of corpus\n",
    "\t'tfidf__min_df': (1, 2, 3), # we need to see a word at least (once, twice, thrice) in a document\n",
    "\t'tfidf__use_idf': (False, True), # use inverse document frequency weighting\n",
    "\t#'tfidf__sublinear_tf': (False, True),\n",
    "\t'tfidf__binary': (False, True), #set term frequency binary (all non-zero terms are set to 1)\n",
    "\t'tfidf__norm': ('l1', 'l2'), #norm used to normalize term vectors\n",
    "\t#'tfidf__max_features': (None, 2000, 5000),\n",
    "\t#'tfidf__stop_words': (None, 'english'),\n",
    "\n",
    "\t#'tfidfchar_ngram_range': ((1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(2,2),(2,3),(2,4),(2,5),(2,6),(3,3),(3,4),(3,5),(3,6),(4,4),(4,5),(4,6),(5,5),(5,6),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(7,7)),\n",
    "\t\n",
    "\t\n",
    "\t'clf__C':(0.1, 1, 5) # penalty parameter for the SVM\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object with the pipeline and hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=3, n_jobs=24, verbose=1)\n",
    "t0 = time()\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 643.560s\n",
      "Best cross-validation score:  0.8599419448476052\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\ttfidf__binary: True\n",
      "\ttfidf__min_df: 1\n",
      "\ttfidf__ngram_range: (2, 3)\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: False\n"
     ]
    }
   ],
   "source": [
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean cross-validated score\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "\tprint(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the best model\n",
      "Accuracy: 0.8598726114649682\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86        79\n",
      "           1       0.85      0.87      0.86        78\n",
      "\n",
      "    accuracy                           0.86       157\n",
      "   macro avg       0.86      0.86      0.86       157\n",
      "weighted avg       0.86      0.86      0.86       157\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[67 12]\n",
      " [10 68]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = sorted(parameters.keys())\n",
    "vec = TfidfVectorizer()\n",
    "clf = LinearSVC()\n",
    "pipeline_test = Pipeline([('tfidf', vec), ('clf', clf)])\n",
    "pipeline_test.set_params(**best_parameters)\n",
    "print('fitting the best model')\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "y_predict = cross_val_predict(pipeline_test, X_train, y_train, cv=3)\n",
    "print('Accuracy:', accuracy_score(y_train, y_predict) )\n",
    "print('\\nClassification report:\\n', classification_report(y_train, y_predict))\n",
    "print('\\nConfusion matrix:\\n', confusion_matrix(y_train, y_predict), '\\n\\n_______________________\\n\\n')\n",
    "accuracies = []\n",
    "accuracies.append(accuracy_score(y_train, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8598726114649682]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Ytest, Ypredict): #evaluate the model (accuracy, precision, recall, f-score, confusion matrix)\n",
    "        print('Accuracy:', accuracy_score(Ytest, Ypredict) )\n",
    "        print('\\nClassification report:\\n', classification_report(Ytest, Ypredict))\n",
    "        print('\\nCR:', precision_recall_fscore_support(Ytest, Ypredict, average='macro'))\n",
    "        print('\\nConfusion matrix:\\n', confusion_matrix(Ytest, Ypredict), '\\n\\n_______________________\\n\\n')\n",
    "        \n",
    "        # Evaluate the performance of the model\n",
    "        accuracy = accuracy_score(Ytest, Ypredict) *100.0\n",
    "        precision = precision_score(Ytest, Ypredict, average='binary')\n",
    "        recall = recall_score(Ytest, Ypredict, average='binary')\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        print(f' Accuracy: {accuracy:.2f} \\n Precision: {precision:.3f} \\n Recall: {recall:.3f} \\n F1: {f_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cross_val(Xtrain, Ytrain, vec, c): #Linear SVC model cross-validation\n",
    "    print('***10-fold cross-validation***')\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion(\n",
    "            [vec],\n",
    "        )),\n",
    "        ('classifier', LinearSVC(C=c))\n",
    "        ])\n",
    "    Ypredict_train = cross_val_predict(pipeline, Xtrain, Ytrain, cv=10) #10-fold cross-validation\n",
    "    evaluate(Ytrain, Ypredict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_test(Xtrain, Ytrain, Xtest_v, Ytest_v, vec, c): #test on 'violations' test set\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([vec]\n",
    "        )),\n",
    "        ('classifier', LinearSVC(C=c))\n",
    "        ])\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    print('***testing on test set***')\n",
    "    Ypredict_test = pipeline.predict(Xtest_v)\n",
    "    evaluate(Ytest_v, Ypredict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1,\n",
       " 'tfidf__binary': True,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (2, 3),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': False}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = ('wordvec', TfidfVectorizer(analyzer = 'word', ngram_range = (2,3), binary = True, min_df = 1, norm = 'l2', use_idf = False))\n",
    "c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***10-fold cross-validation***\n",
      "Accuracy: 0.8662420382165605\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87        79\n",
      "           1       0.88      0.85      0.86        78\n",
      "\n",
      "    accuracy                           0.87       157\n",
      "   macro avg       0.87      0.87      0.87       157\n",
      "weighted avg       0.87      0.87      0.87       157\n",
      "\n",
      "\n",
      "CR: (0.8668292682926829, 0.8661148977604674, 0.86615515771526, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[70  9]\n",
      " [12 66]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      " Accuracy: 86.62 \n",
      " Precision: 0.880 \n",
      " Recall: 0.846 \n",
      " F1: 0.863\n"
     ]
    }
   ],
   "source": [
    "train_model_cross_val(X_train, y_train, vec, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***testing on test set***\n",
      "Accuracy: 0.9137447405329593\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.95      1406\n",
      "           1       0.11      0.75      0.20        20\n",
      "\n",
      "    accuracy                           0.91      1426\n",
      "   macro avg       0.55      0.83      0.58      1426\n",
      "weighted avg       0.98      0.91      0.94      1426\n",
      "\n",
      "\n",
      "CR: (0.5544574894312347, 0.8330369843527738, 0.5752529985688236, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[1288  118]\n",
      " [   5   15]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      " Accuracy: 91.37 \n",
      " Precision: 0.113 \n",
      " Recall: 0.750 \n",
      " F1: 0.196\n"
     ]
    }
   ],
   "source": [
    "train_model_test(X_train, y_train, X_test, y_test, vec, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***testing on test set***\n",
      "Accuracy: 0.8\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81        20\n",
      "           1       0.83      0.75      0.79        20\n",
      "\n",
      "    accuracy                           0.80        40\n",
      "   macro avg       0.80      0.80      0.80        40\n",
      "weighted avg       0.80      0.80      0.80        40\n",
      "\n",
      "\n",
      "CR: (0.803030303030303, 0.8, 0.7994987468671679, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[17  3]\n",
      " [ 5 15]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      " Accuracy: 80.00 \n",
      " Precision: 0.833 \n",
      " Recall: 0.750 \n",
      " F1: 0.789\n"
     ]
    }
   ],
   "source": [
    "train_model_test(X_train, y_train, X_test1, y_test1, vec, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***testing on test set***\n",
      "Accuracy: 0.9170274170274171\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96      1386\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.92      1386\n",
      "   macro avg       0.50      0.46      0.48      1386\n",
      "weighted avg       1.00      0.92      0.96      1386\n",
      "\n",
      "\n",
      "CR: (0.5, 0.45851370851370854, 0.47835905156191194, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[1271  115]\n",
      " [   0    0]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      " Accuracy: 91.70 \n",
      " Precision: 0.000 \n",
      " Recall: 0.000 \n",
      " F1: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_34916\\659736298.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f_score = 2 * (precision * recall) / (precision + recall)\n"
     ]
    }
   ],
   "source": [
    "train_model_test(X_train, y_train, X_test2, y_test2, vec, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
