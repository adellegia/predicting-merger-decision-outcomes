{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from pandas.core.common import flatten\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and pre-processing\n",
    "import glob, re, os, sys, random\n",
    "from random import shuffle\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector representations and embeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling - Logistic, XGBOOST, SVM\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM \n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT models\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(r\"../../../data/processed/pre-processed_2023_03_11.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1 vs. Phase 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling minority class by balancing based on the number of cases and number of sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['phase2'].isin([0,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase2\n",
      "0    1485\n",
      "1      98\n",
      "Name: case_num, dtype: int64 \n",
      " 0    5079\n",
      "1     292\n",
      "Name: phase2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df1.groupby('phase2')['case_num'].nunique(), \"\\n\",\n",
    "df1['phase2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase2  section_fin              \n",
      "0       Competitive Assessment       1256\n",
      "        Concentration & Dimension    1467\n",
      "        Market Definition             970\n",
      "        Parties & Operation          1376\n",
      "1       Competitive Assessment         75\n",
      "        Concentration & Dimension      71\n",
      "        Market Definition              81\n",
      "        Parties & Operation            65\n",
      "Name: case_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of cases per unique combination of phase2 and section_fin\n",
    "case_counts = df1.groupby(['phase2', 'section_fin'])['case_num'].nunique()\n",
    "print(case_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (3222,) (3222, 2)\n",
      "Validation set shape: (1074,) (1074, 2)\n",
      "Test set shape: (1075,) (1075, 2)\n"
     ]
    }
   ],
   "source": [
    "# Separate the data into features (text) and labels (phase and section_fin)\n",
    "X = df1['text_clean']\n",
    "y = df1[['phase2', 'section_fin']]\n",
    "case_num = df1['case_num']\n",
    "\n",
    "# Define the number of splits and the test size\n",
    "n_splits = 1\n",
    "test_size = 0.2\n",
    "\n",
    "# Initialize the StratifiedShuffleSplit object\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train_val, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_val, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    case_num_train_val, case_num_test = case_num.iloc[train_index], case_num.iloc[test_index]\n",
    "\n",
    "# Further split the training_validation set into 75% training and 25% validation sets\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25, random_state=42)\n",
    "for train_index, val_index in sss.split(X_train_val, y_train_val):  # <-- Use X_train_val and y_train_val\n",
    "    X_train, X_val = X_train_val.iloc[train_index], X_train_val.iloc[val_index]\n",
    "    y_train, y_val = y_train_val.iloc[train_index], y_train_val.iloc[val_index]\n",
    "    case_num_train, case_num_val = case_num_train_val.iloc[train_index], case_num_train_val.iloc[val_index]\n",
    "\n",
    "# Print the shape of each set to verify that the data has been split correctly\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Separate the data into features (text) and labels (phase and section_fin)\n",
    "# X = df1['text_clean']\n",
    "# y = df1[['phase2', 'section_fin']]\n",
    "# case_num = df1['case_num']\n",
    "\n",
    "# # Split the data into train and test sets based on phase2 and section_fin\n",
    "# X_train_val, X_test, y_train_val, y_test, case_num_train_val, case_num_test = train_test_split(X, y, case_num, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# # Further split the train_validation set into train and validation sets based on section_fin label distribution\n",
    "# X_train, X_val, y_train, y_val, case_num_train, case_num_val = train_test_split(X_train_val, y_train_val, case_num_train_val, test_size=0.25, stratify=y_train_val, random_state=42)\n",
    "\n",
    "# # Print the shape of each set to verify that the data has been split correctly\n",
    "# print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "# print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "# print(\"Test set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X_train_val, y_train_val, and case_num_train_val along axis=1\n",
    "train_val_df = pd.concat([X_train_val, y_train_val, case_num_train_val], axis=1)\n",
    "\n",
    "# Concatenate X_train, y_train, and case_num_train along axis=1\n",
    "train_df = pd.concat([X_train, y_train, case_num_train], axis=1)\n",
    "\n",
    "# Concatenate X_val, y_val, and case_num_val along axis=1\n",
    "test_df = pd.concat([X_val, y_val, case_num_val], axis=1)\n",
    "\n",
    "# Concatenate X_test, y_test, and case_num_test along axis=1\n",
    "val_df = pd.concat([X_test, y_test, case_num_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phase2  section_fin              \n",
       "0       Competitive Assessment       754\n",
       "        Concentration & Dimension    881\n",
       "        Market Definition            581\n",
       "        Parties & Operation          825\n",
       "1       Competitive Assessment        45\n",
       "        Concentration & Dimension     43\n",
       "        Market Definition             49\n",
       "        Parties & Operation           39\n",
       "Name: case_num, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['phase2', 'section_fin'])['case_num'].nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize X features in train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversample minority class (phase2=1) using vectorized X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m# Use RandomOverSampler to oversample the minority class based on the desired number of samples for each section_fin value\u001b[39;00m\n\u001b[0;32m     27\u001b[0m oversampler \u001b[39m=\u001b[39m RandomOverSampler(sampling_strategy\u001b[39m=\u001b[39mdesired_samples_per_section_fin, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m X_resampled, y_resampled \u001b[39m=\u001b[39m oversampler\u001b[39m.\u001b[39;49mfit_resample(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\imblearn\\base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 203\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\imblearn\\base.py:80\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_resample\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     check_classification_targets(y)\n\u001b[0;32m     81\u001b[0m     arrays_transformer \u001b[39m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m     82\u001b[0m     X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X_y(X, y)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\multiclass.py:192\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_classification_targets\u001b[39m(y):\n\u001b[0;32m    181\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Ensure that target y is of a non-regression type.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[39m    Only the following target types (as defined in type_of_target) are allowed:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m        Target values.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     y_type \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultilabel-sequences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    199\u001b[0m     ]:\n\u001b[0;32m    200\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\multiclass.py:286\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m sparse_pandas:\n\u001b[0;32m    284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be class \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseSeries\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseArray\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 286\u001b[0m \u001b[39mif\u001b[39;00m is_multilabel(y):\n\u001b[0;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m \u001b[39m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\multiclass.py:173\u001b[0m, in \u001b[0;36mis_multilabel\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    165\u001b[0m         \u001b[39mlen\u001b[39m(y\u001b[39m.\u001b[39mdata) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39munique(y\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         )\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munique(y)\n\u001b[0;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(labels) \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m (\n\u001b[0;32m    176\u001b[0m         y\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mbiu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m _is_integral_float(labels)  \u001b[39m# bool, int, uint\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     )\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\thesis\\lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     ar\u001b[39m.\u001b[39;49msort()\n\u001b[0;32m    337\u001b[0m     aux \u001b[39m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mbool_)\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Calculate the number of unique cases for each section_fin value in the minority class phase2=1\n",
    "minority_class_data = train_df[train_df['phase2'] == 1]\n",
    "unique_cases_per_section_fin = minority_class_data.groupby('section_fin')['case_num'].nunique()\n",
    "\n",
    "# Calculate the number of unique cases for each section_fin value in the majority class phase2=0\n",
    "majority_class_data = train_df[train_df['phase2'] == 0]\n",
    "unique_cases_per_section_fin_majority = majority_class_data.groupby('section_fin')['case_num'].nunique()\n",
    "\n",
    "# Calculate the minimum number of unique cases among all section_fin values\n",
    "min_unique_cases = unique_cases_per_section_fin.min()\n",
    "\n",
    "# Calculate the desired number of samples for each section_fin value in the minority class\n",
    "desired_samples_per_section_fin = {}\n",
    "for section_fin_value in unique_cases_per_section_fin.index:\n",
    "    num_unique_cases = unique_cases_per_section_fin.loc[section_fin_value]\n",
    "    desired_samples_per_section_fin[section_fin_value] = int(min_unique_cases / num_unique_cases)\n",
    "\n",
    "# Adjust the desired number of samples based on the number of unique cases in the majority class for each section_fin value\n",
    "for section_fin_value in unique_cases_per_section_fin.index:\n",
    "    if section_fin_value in unique_cases_per_section_fin_majority.index:\n",
    "        num_unique_cases_majority = unique_cases_per_section_fin_majority.loc[section_fin_value]\n",
    "        desired_samples_per_section_fin[section_fin_value] = min(desired_samples_per_section_fin[section_fin_value], num_unique_cases_majority)\n",
    "\n",
    "# Use RandomOverSampler to oversample the minority class based on the desired number of samples for each section_fin value\n",
    "oversampler = RandomOverSampler(sampling_strategy=desired_samples_per_section_fin, random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_cases_majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Competitive Assessment': 0,\n",
       " 'Concentration & Dimension': 0,\n",
       " 'Market Definition': 0,\n",
       " 'Parties & Operation': 1}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_samples_per_section_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "section_fin\n",
       "Competitive Assessment       45\n",
       "Concentration & Dimension    43\n",
       "Market Definition            49\n",
       "Parties & Operation          39\n",
       "Name: case_num, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cases_per_section_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_unique_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Competitive Assessment': 0,\n",
       " 'Concentration & Dimension': 0,\n",
       " 'Market Definition': 0,\n",
       " 'Parties & Operation': 1}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_samples_per_section_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Define the resampling strategy for each section\n",
    "# Note that the ratios here are just examples and you may need to adjust them based on your specific data\n",
    "sampling_strategy = {'Competitive Assessment': 0.5, 'Concentration & Dimension': 0.6, 'Market Definition': 0.8, 'Parties & Operation': 0.7}\n",
    "\n",
    "# Create the RandomOverSampler object with the desired sampling strategy\n",
    "oversampler = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "\n",
    "# Resample the data\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "df_resampled = pd.DataFrame({'text_clean': X_resampled, 'phase': y_resampled})\n",
    "\n",
    "# You can now use the resampled data for training your machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8a34a3bb875764d64c764b9cffa891f179e7153a1578da198ecc075e870264c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
